1. William Fleetwood & Brooks Sparling

2. The program runs as is expected, but does so at a less accurate rate. The example output shows ~30% accuracy where our program only achieves ~15%. We think this is an issue with the calculation of the bigram probabilities, however we have been unable to locate specifically what the issue is that causes this lessened accuracy.

3. To the best of our knowledge, our program runs efficiently both in regards to space complexity and time. Originally, we had implemented the .get() command inside of and iterative loop, which we later determined was greatly inefficient for time. We fixed this issue using an Iterator instead, decreasing the runtime from O(n^2) to O(n). Our implementation of Wordifier does establish several instances of sets which may be possible to do more efficiently in regards to space complexity, although we were unable to do so. 

4. The most challenging aspect of this project was choosing the correct structure to decrease the runtime to O(n). Working with large sets of data and incorporating run time and space complexity into planning are new concepts and were challenging. Having such a long runtime made debugging difficult as you have to wait for the program to finish before you can tell what is wrong. Even determining where the bug is happening becomes more complex when you are working with and selectively reformatting a large dataset.

5. We began with test.txt to test if our program was properly importing data, building the structures we wanted, formatting correctly and ultimately would allow us to test on larger documents when we had proved the validity of these things. After working out a few bugs and seeing that we had proved these things, we moved on to test1.txt which was fairly repetitive in content, to test how our program ran with high occurrences of the same unigrams. Finally, using the documents provided we tested that the program could run on longer documents at an efficient asymptotic rate. As we had predicted based on the quick rate on the smaller documents, it runs the full length of grimms.txt in a matter of a few seconds. 

6. Our program seems to be most efficiently running on the document wizard.txt, with a count threshold of 2 and probability threshold of 0.07, finding  ~18% of the dictionary words. The program finds the highest number of words reading shakespeare.txt at ~2800 words, however the percent words found is lower.
